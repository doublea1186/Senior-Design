import torch
from sentence_transformers import SentenceTransformer, util
from torch import Tensor
from backend.utils.constants import CUDA

# pick one
# model = SentenceTransformer('all-mpnet-base-v2')  # this is higher quality
model = SentenceTransformer('all-MiniLM-L6-v2')  # this is faster
if CUDA:
    model = model.cuda()


def multiple_embeddings(sentences: list[str]) -> Tensor:
    """
    Compute vector embeddings for multiple sentences at the same time.
    Recommended over sentence_embedding for performance
    :param sentences: The text sentences to be turned into vectors
    :return: A Tensor containing the vector representations of the sentences
    """
    tensor = model.encode(sentences, convert_to_tensor=True)
    if CUDA:
        return tensor.cuda()
    else:
        return tensor


def sentence_embedding(sentence: str) -> Tensor:
    """
    Computes the vector embedding of a single sentence. Slower than
    multiple_embeddings if used repeatedly instead of a single call to
    that function.
    :param sentence: The text sentence to be turned into a vector
    :return: A Tensor containing the vector representation of the sentence
    """
    tensor = model.encode([sentence], convert_to_tensor=True)[0]
    if CUDA:
        return tensor.cuda()
    else:
        return tensor


def single_cos_sim(embedding1: Tensor, embedding2: Tensor) -> float:
    """
    Computes the cosine similarity of two single sentence embeddings.
    Slower than multi_cos_sim if multiple calls are needed.
    :param embedding1: The first sentence embedding
    :param embedding2: The second sentence embedding
    :return: A float denoting how similar the two sentences are (higher means
        more similar)
    """
    return util.cos_sim(embedding1, embedding2).item()


def multi_cos_sim(embeddings1: Tensor, embeddings2: Tensor) -> Tensor:
    """
    Computes the pairwise cosine similarity of multiple sentence embeddings.
    Faster than multiple calls of single_cos_sim.
    :param embeddings1: A tensor containing n embeddings
    :param embeddings2: A tensor containing m embeddings
    :return: An n x m Tensor containing pairwise cosine similarities for all
        embedding pairs
    """
    return util.cos_sim(embeddings1, embeddings2)


def sims_from_sentences(s1: list[str], s2: list[str]) -> Tensor:
    """
    Computes pairwise similarities based on two sentence lists
    :param s1: The first sentence list of length n
    :param s2: The second sentence list of length m
    :return: An n x m Tensor containing pairwise cosine similarities for all
        embedding pairs
    """
    e1 = multiple_embeddings(s1)
    e2 = multiple_embeddings(s2)
    return multi_cos_sim(e1, e2)


def max_sim(t1: Tensor, t2: Tensor) -> float:
    flattened_sims = multi_cos_sim(t1, t2).flatten()
    return torch.max(flattened_sims).item()


def sim_from_multi_matrix(matrix: Tensor, i1: int, i2: int) -> float:
    """
    Get a single cosine similarity from a similarity matrix generated by
    multi_cosine_sim.
    :param matrix: The matrix of pairwise cosine similarities
    :param i1: The index of the first sentence embedding
    :param i2: The index of the second sentence embedding
    :return: A float representing the cosine similarities of the two embeddings
    """
    return matrix[i1][i2].item()


if __name__ == '__main__':
    # here's a demo of the embeddings at work
    embeddings = multiple_embeddings(
        ['The cat sits outside', 'A man is playing guitar',
         'The new movie is awesome'])
    print(sim_from_multi_matrix(multi_cos_sim(embeddings, embeddings), 0, 1))
