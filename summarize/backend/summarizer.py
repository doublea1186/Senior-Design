import locale
import os
import time
import openai
import json
from summarize.backend.utils.constants import MAX_ALLOWED_TOKENS, SUMMARY_TOKEN_TARGET, \
    TARGET_FEW_SHOT_EXAMPLES
from summarize.backend.utils.query_openai import ask_openai
from summarize.backend.utils.results_chunks import Chunk, ResultChunk, Document
from summarize.backend.utils.sentence_splitter import split_into_sentences
from summarize.backend.utils.text_utils import num_tokens, text_to_chunks
from typing import Optional


starter_text = ("I am a summarization bot. If you give me text, "
                "I will provide a textbook-like summary "
                "without repeating past summaries or describing the speaker.")
divider = '\n\n###\n\n'


def summarize_using_prompt(chunk_text: str, built_prompt: str,
                           verbose=False) -> Optional[ResultChunk]:
    """
    Creates a summary using a pre-built prompt
    :param chunk_text: The text that is being summarized
    :param built_prompt: The prompt used for the GPT-3 query.
        Should be of the form:
        <starter_text>
        ###
        <few-shot examples separated by ###>
        ###
        <prompt>
        <response>
    :param verbose: whether to print additional info during execution
    :return: A ResponseChunk object containing information on the generation
        process and result, if there was one
    """
    prompt_chunk = Chunk(built_prompt)
    if verbose:
        print(f'Prompt is {prompt_chunk.tokens} tokens:\n'
              f'=== PROMPT STARTS HERE ===')
        print(built_prompt)
        print(f'=== PROMPT ENDS HERE ===')
    if prompt_chunk.tokens + SUMMARY_TOKEN_TARGET >= MAX_ALLOWED_TOKENS:
        raise ValueError(f'Not able to generate a summary - prompt allows for '
                         f'{MAX_ALLOWED_TOKENS - prompt_chunk.tokens} tokens '
                         f'in output, but target is {SUMMARY_TOKEN_TARGET}')
    else:
        max_response_tokens = MAX_ALLOWED_TOKENS - prompt_chunk.tokens
        summarized = ask_openai(built_prompt,
                                max_tokens=max_response_tokens,
                                stop=['###'])
        if verbose:
            print(f'=== OUTPUT STARTS HERE ===\n'
                  f'{summarized}\n'
                  f'=== OUTPUT ENDS HERE ===')
        result_obj = ResultChunk(chunk_text, prompt_chunk, Chunk(summarized))

        return result_obj


def summarize_zero_shot(chunk_text: str, prev_outputs: set[str],
                        fine_tuned=False,
                        verbose=False) -> Optional[ResultChunk]:
    """
    Attempts to summarize the given text without giving GPT-3 any prior context.
    :param chunk_text: The text to summarize
    :param prev_outputs: A set tracking all summaries generated by the model.
        Used to regenerate down the line if needed.
    :param verbose: Whether print statements should occur along the way
    :return: A ResponseChunk object containing information on the generation
        process and result, if there was one
    """
    if fine_tuned:
        prompt = (f"Text:\n\n"
                  f"{chunk_text}\n\n"
                  f"Summary:")
    else:
        prompt = (f"{starter_text}\n\n"
                  f"###\n\n"
                  f"Text:\n\n"
                  f"{chunk_text}\n\n"
                  f"Summary:")
    output = summarize_using_prompt(chunk_text, prompt, verbose)
    prev_outputs.add(output.response.text)
    return output


def summarize_auto_context(chunk_text: str,
                           prev_results: list[ResultChunk],
                           prev_outputs: set[str],
                           fine_tuned=False,
                           verbose=False) -> Optional[ResultChunk]:
    """
    Summarizes the given text using few-shot learning on GPT-3.
    Similarity calculations work like this:
        1. Compute zero-shot topics for the given chunk
        2. If there are exact matches in topic_to_chunks, use those other
           chunks + results as context.
        3. Otherwise, look for chunks with maximum topic vector similarity.
    Add the most similar past summaries as context until either:
        1. We have MAX_FEW_SHOT_EXAMPLES past summaries, or
        2. Adding another example would not let us leave SUMMARY_TOKEN_LIMIT
           tokens for the current chunk's generated summary.
    The new contextualized summary is the actual chunk summary.
    :param chunk_text: The text to be summarized
    :param prev_results: ResultChunk objects that resulted from summarizing
        previous sections of the text
    :param prev_outputs: A set tracking all summaries generated by the model.
        Used to regenerate down the line if needed.
    :param verbose: Whether print statements should occur along the way
    :return: A ResponseChunk object containing information on the generation
        process and result, if there was one
    """
    num_prev_results = len(prev_results)
    context_examples: list[ResultChunk] = \
        prev_results[max(0, num_prev_results - TARGET_FEW_SHOT_EXAMPLES):]

    last_section = (f'Text:\n\n'
                    f'{chunk_text}\n\n'
                    f'Summary:')

    if fine_tuned:
        context_section = '\n'.join([chunk.response.text
                                     for chunk in context_examples])
        prompt = (f"Previous summaries:\n\n"
                  f"{context_section}\n\n"
                  f"{last_section}")
    else:
        prompt_sections = [starter_text]
        prompt_sections.extend(c.text for c in context_examples)
        if verbose:
            print(f'prompt sections: {prompt_sections}')
        prompt = None
        for i in range(1, len(prompt_sections) + 1):
            sections = prompt_sections[0:i]
            sections.append(last_section)
            new_prompt = divider.join(sections)
            if num_tokens(
                    new_prompt) + SUMMARY_TOKEN_TARGET <= MAX_ALLOWED_TOKENS:
                prompt = new_prompt
        if prompt is None:
            raise ValueError('No valid prompt possible')

    first_try = summarize_using_prompt(chunk_text, prompt, verbose)
    if first_try.response.text not in prev_outputs:
        prev_outputs.add(first_try.response.text)
        return first_try
    else:
        if verbose:
            print('First summarization produced duplicate. Retrying...')
        second_try = summarize_using_prompt(chunk_text, prompt, verbose)
        # account for tokens used in first attempt for final accounting
        second_try.tokens += first_try.tokens
        if second_try.response.text not in prev_outputs:
            prev_outputs.add(second_try.response.text)
            return second_try
        else:
            print('First and second summarizations were duplicates. '
                  'Using zero-shot instead.')
            # just summarize with no context
            final_try = summarize_zero_shot(chunk_text, prev_outputs, verbose)
            # account for tokens used in first/second attempts
            final_try.tokens += second_try.tokens
            prev_outputs.add(final_try.response.text)
            return final_try


def summarize_chunks(chunks: list[str], fine_tuned=False, verbose=False) -> \
        Optional[Document]:
    """
    Summarizes the given chunks one at a time.
    1. Summarize the first chunk with zero-shot
    2. For each following chunk, use few-shot
    3. (optional) Do a second pass - this time every summarization has maximal
       few-shot examples.
    :param chunks: The list of chunks to be summarized
    :param verbose: Whether extra output should be displayed or not
    :return: A list of summarized chunks (or None if chunks was None) along with
        the number of tokens used by the summarization process
    """
    if chunks is None or len(chunks) == 0:
        return chunks
    results: list[ResultChunk] = []
    prev_outputs: set[str] = set()

    results.append(summarize_zero_shot(chunks[0], prev_outputs, fine_tuned,
                                       verbose))

    mark = time.perf_counter()
    for chunk in chunks[1:]:
        results.append(summarize_auto_context(chunk, results, prev_outputs,
                                              fine_tuned, verbose))
        next_time = time.perf_counter()
        if verbose:
            print(f'ELAPSED: {next_time - mark}')
        mark = next_time
    return Document(results)


# TODO: separate summarization function to fit fine-tuned prompt structure


def create_path_if_needed(path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)


def summarize_file(transcript_path: str, summary_path: Optional[str] = None,
                   out_csv_path: Optional[str] = None,
                   out_finetune_path: Optional[str] = None,
                   encoding: str = locale.getpreferredencoding(False),
                   config_path='../config.txt', trim_divisor=-1,
                   fine_tuned=False, verbose=False) -> None:
    """
    Summarizes a transcript using the given parameters
    :param transcript_path: The path to the raw transcript
    :param summary_path: the path to a txt containing the summary
    :param out_finetune_path: Where the fine-tuning data should be output to.
        Should be a JSONL file for GPT-3
    :param encoding: The encoding of the transcript, if file loading
        will break otherwise (e.g. UTF-8)
    :param config_path: Path to the personal hidden config
    :param trim_divisor: If less than the full transcript should be summarized,
        this is n where the first 1/n portion of the transcript will be used
    :param fine_tuned: True if this function call is being used in the context
        of the fine-tuned model prompt structure created by CCB. Passed along
        to other functions.
    :param verbose: Whether additional output should be printed
    """
    with open(transcript_path, encoding=encoding) as transcript:
        if 'OPENAI_API_KEY' in os.environ.keys():
            openai.api_key = os.environ['OPENAI_API_KEY']
        else:
            with open(config_path) as config:
                openai.api_key = config.readline().strip()
        lines = transcript.readlines()
        if len(lines) > 1:
            text = ''.join(lines).splitlines()
            # remove extra line breaks with list comprehension
            # then combine all lines into one
            text = ' '.join(x for x in text if x)
        else:
            # is single line
            text = lines[0]

        if verbose or trim_divisor != -1:
            sentences = split_into_sentences(text)
            if verbose:
                print(f'Sentences in original text: {len(sentences)}')
            if trim_divisor != -1:
                num_to_summarize = len(sentences) // trim_divisor
                trimmed = sentences[:len(sentences) // trim_divisor]
                if verbose:
                    print(f'Summarizing first '
                          f'{num_to_summarize / len(sentences) * 100}% '
                          f'of the transcript')
                text = '\n'.join(trimmed)

        document = summarize_chunks(text_to_chunks(text), fine_tuned=fine_tuned,
                                    verbose=verbose)
        print(f'Cost: {document.cost}')

        if verbose:
            print('=== Final summary ===')
            print(document.summary)
            print(f'Input length: {len(text)}')
            print(f'Output length: {len(document.summary)}')

        if summary_path is not None:
            create_path_if_needed(summary_path)
            with open(summary_path, 'w', encoding='utf8') as out_file:
                out_file.write(document.summary)

        if out_csv_path is not None:
            create_path_if_needed(out_csv_path)
            with open(out_csv_path, 'w', encoding='utf8') as csv_file:
                csv_file.write('input,output\n')
                for chunk in document.results:
                    csv_file.write(f'{chunk.transcript},{chunk.response.text}'
                                   f'\n')

        if out_finetune_path is not None:
            create_path_if_needed(out_finetune_path)
            fine_tune_data = []
            for chunk in document.results:
                fine_tune_data.append({'prompt': chunk.prompt.text,
                                       'completion': chunk.response.text})
            with open(out_finetune_path, 'w', encoding='utf8') \
                    as fine_tune_file:
                for example in fine_tune_data:
                    fine_tune_file.write(json.dumps(example))
                    fine_tune_file.write('\n')


if __name__ == '__main__':
    summarize_file('../resources/quality-control.txt',
                   '../resources/quality-control-out.txt'
                   '../resources/cached-qc-fine-tune-run/qc-fine-tune.jsonl',
                   trim_divisor=5, verbose=True,
                   fine_tuned=True,
                   encoding='utf8')
